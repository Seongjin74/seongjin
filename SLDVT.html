<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SLDVT – 수화 감지 음성 번역기</title>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap"
    rel="stylesheet"
  />
  <!-- style.css 파일 참조 -->
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header class="site-header">
    <div class="container header-container">
      <div class="logo">
        <a href="#">Seongjin's Portfolio</a>
      </div>
      <nav class="site-nav">
        <ul>
          <li><a href="#about" id="navAbout">소개</a></li>
          <li><a href="#content" id="navContent">프로젝트 상세</a></li>
          <li><a href="#contact" id="navContact">연락하기</a></li>
        </ul>
      </nav>
      <div class="header-controls">
        <button id="darkModeToggle" title="다크모드 전환">🌙</button>
        <button id="langToggle" class="lang-btn" title="언어 전환">ENG</button>
      </div>
    </div>
  </header>

  <main>
    <!-- 프로젝트 소개 섹션 -->
    <section id="about" class="section about">
      <div class="container">
        <h1>프로젝트: SLDVT – 수화 감지 음성 번역기</h1>
        <p>
          본 프로젝트는 깊이 있는 학습 기법 중 하나인 LSTM(Long Short-Term Memory) 신경망을 활용하여, 수화 제스처를 실시간 음성으로 변환하는 시스템을 개발하는 것을 목표로 합니다. 이를 통해 청각 장애인과 비장애인 사이의 소통 격차를 줄이고, 모두가 보다 원활하게 소통할 수 있도록 돕고자 합니다.
        </p>
        <p>
          시스템은 카메라나 센서를 통해 사용자의 수화 제스처를 실시간으로 캡처합니다. 캡처된 영상은 LSTM 기반의 행동 인식 모델을 통해 분석되며, 이 모델은 다양한 정적 및 동적 제스처를 학습하여 인식할 수 있도록 설계되었습니다. LSTM 네트워크는 시퀀스 데이터의 시간적 의존성을 효과적으로 모델링하여 수화의 동적인 특성을 잘 반영합니다.
        </p>
        <p>
          인식된 제스처는 텍스트 정보로 변환된 후, 텍스트-투-스피치(TTS) 기술을 이용해 음성으로 합성됩니다. 합성된 음성은 스피커나 헤드폰을 통해 출력되어, 수화를 모르는 사람들과도 원활한 소통이 가능하도록 합니다.
        </p>
        <ul>
          <li><strong>실시간 수화 인식</strong>: 지연 없이 즉각적으로 제스처를 인식하여 소통이 원활합니다.</li>
          <li><strong>다중 제스처 인식</strong>: 정적 제스처와 동적 제스처 모두를 인식할 수 있도록 학습되었습니다.</li>
          <li><strong>정확성과 견고성</strong>: 다양한 환경과 조명 조건에서도 높은 정확도를 유지할 수 있도록 다양한 데이터셋을 활용해 학습하였습니다.</li>
          <li><strong>접근성</strong>: 수화 숙련도에 상관없이 누구나 쉽게 사용할 수 있도록 설계되었습니다.</li>
        </ul>
        <p>
          이러한 시스템은 청각 장애인 및 난청인들이 사회적, 직업적 환경에서 신뢰할 수 있는 소통 수단을 제공받도록 지원하며, 깊이 있는 학습 기법과 LSTM 네트워크의 역량을 통해 소통의 장벽을 허무는 것을 목표로 합니다.
        </p>
      </div>
    </section>

    <!-- 아키텍처 및 워크플로우 섹션 -->
    <section id="content" class="section">
      <div class="container">
        <h2>아키텍처 및 워크플로우</h2>
        <h3>High Level Diagram</h3>
        <p>상위 수준의 시스템 구조를 나타낸 다이어그램입니다.</p>
        <img src="HLD.png" alt="High Level Diagram" style="max-width:100%; margin-bottom: 1rem;" />
        <h3>Workflow</h3>
        <p>시스템의 동작 과정 및 데이터 흐름을 시각적으로 설명한 워크플로우입니다.</p>
        <img src="Workflow.jpeg" alt="Workflow Diagram" style="max-width:100%;" />
      </div>
    </section>

    <!-- 시스템 세부 설명 섹션 -->
    <section id="system-details" class="section">
      <div class="container">
        <h2>시스템 세부 설명</h2>
        <h3>LSTM 모델 개요</h3>
        <p>
          LSTM은 인공 신경망의 한 종류로, 시퀀스 데이터를 입력받아 원하는 출력을 생성하는 모델입니다. 기존의 순환 신경망(RNN)은 장기 의존성 문제를 가지고 있었으나, LSTM은 셀 상태(cell state)라는 구조를 도입해 이 문제를 효과적으로 해결하였습니다. 셀 상태는 네트워크의 여러 시점에 걸쳐 정보를 유지하고 전달하여, 장기간의 데이터를 기억할 수 있도록 합니다.
        </p>
        <h3>데이터 수집 및 전처리</h3>
        <p>
          <strong>데이터 수집:</strong> OpenCV API를 이용해 카메라로 30프레임의 데이터를 수집합니다.
        </p>
        <p>
          <strong>데이터 전처리:</strong> MediaPipe를 활용하여 얼굴, 왼손, 오른손, 그리고 몸의 좌표값(총 1662개)을 추출하고, 이를 NumPy 배열로 전처리합니다.
        </p>
        <p>
          <strong>학습:</strong> 전처리된 데이터를 바탕으로 LSTM 모델을 200 epochs 동안 훈련하며, 전체 훈련 과정을 30회 반복합니다.
        </p>
      </div>
    </section>

    <!-- 사용자 모드와 학습 모드 섹션 -->
    <section id="modes" class="section">
      <div class="container">
        <h2>사용자 모드와 학습 모드</h2>
        <h3>Learning 모드</h3>
        <ol>
          <li><strong>입력:</strong> 사용자가 학습할 단어를 입력합니다.</li>
          <li><strong>데이터 저장:</strong> 입력된 단어에 해당하는 폴더를 생성해 학습 데이터를 저장합니다.</li>
          <li><strong>캡처 및 전처리:</strong> OpenCV와 MediaPipe를 이용해 30프레임의 수화 데이터를 캡처하고, 전처리 후 저장합니다.</li>
          <li><strong>학습:</strong> 데이터를 8:2의 비율로 훈련과 검증 데이터로 분할하여 LSTM 모델을 학습시킵니다.</li>
          <li><strong>모델 저장:</strong> 학습된 모델은 <code>action.keras</code> 파일에 저장됩니다.</li>
        </ol>
        <h3>Motion Capture(사용자) 모드</h3>
        <ol>
          <li><strong>실시간 캡처:</strong> OpenCV를 통해 사용자의 수화 동작을 실시간으로 캡처합니다.</li>
          <li><strong>랜드마크 추출:</strong> MediaPipe로 손, 얼굴, 포즈 등의 랜드마크를 추출하고, 신뢰도가 높은 데이터만 선별합니다.</li>
          <li><strong>데이터 누적:</strong> 일정한 시퀀스 길이(예: 30프레임)로 데이터를 누적합니다.</li>
          <li><strong>예측:</strong> 학습된 LSTM 모델에 누적된 데이터를 입력하여 예측을 수행합니다. 예측 결과 중 가장 높은 확률의 단어를 선택합니다.</li>
          <li><strong>출력 조건:</strong> 최근 10개의 예측 결과가 모두 동일하며, 해당 예측의 확률이 95%를 초과하고 이전 예측 단어와 다를 경우에만 최종 단어가 음성으로 출력됩니다.</li>
          <li><strong>음성 변환:</strong> Google Text-to-Speech API를 사용해 예측 단어를 음성으로 변환하고, 스피커를 통해 재생합니다.</li>
          <li><strong>종료:</strong> 사용자가 종료 명령을 내리면 시스템이 종료됩니다.</li>
        </ol>
      </div>
    </section>

    <!-- LSTM 선택 이유 섹션 -->
    <section id="lstm-reason" class="section">
      <div class="container">
        <h2>LSTM 선택 이유 및 고려사항</h2>
        <p>
          LSTM은 자연어 처리, 음성 인식, 제스처 인식 등 다양한 분야에서 우수한 성능을 보여온 모델입니다. 그 장점으로는 시퀀스 데이터 처리 능력과 장기 의존성 문제 해결 능력이 있으나, 단점으로는 모델 복잡도가 높아 학습 시간과 연산 자원이 많이 소요될 수 있다는 점이 있습니다.
        </p>
        <p>
          현재 다른 모델(예: Transformer, 합성곱 신경망, 양방향 LSTM)도 고려 중이나, 우선 기본 데모 단계에서는 LSTM을 사용해 mac 환경에서 dmg 파일로 배포하는 수준에 도달했습니다. 다만, 정확도 향상을 위해 프레임 수, 예측 임계값 등의 최적화 작업이 추가로 필요하며, TensorFlow Keras 대신 PyTorch를 사용할 경우 동적 계산 그래프의 장점을 활용할 수 있을 것으로 예상됩니다.
        </p>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>&copy; 2025 Seongjin. All Rights Reserved.</p>
    </div>
  </footer>

  <button id="btnTop" title="최상단으로 이동">&#8679;</button>

  <script>
    // 다크 모드 토글
    const darkModeToggle = document.getElementById('darkModeToggle');
    const currentTheme = localStorage.getItem('theme');
    if (currentTheme === 'dark') {
      document.body.classList.add('dark-mode');
      darkModeToggle.textContent = '☀️';
    }
    darkModeToggle.addEventListener('click', () => {
      document.body.classList.toggle('dark-mode');
      if (document.body.classList.contains('dark-mode')) {
        localStorage.setItem('theme', 'dark');
        darkModeToggle.textContent = '☀️';
      } else {
        localStorage.setItem('theme', 'light');
        darkModeToggle.textContent = '🌙';
      }
    });

    // 최상단 버튼
    const btnTop = document.getElementById('btnTop');
    window.addEventListener('scroll', () => {
      btnTop.style.display = window.pageYOffset > 300 ? 'block' : 'none';
    });
    btnTop.addEventListener('click', () => {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });

    // 언어 전환 (간단한 예시)
    const translations = {
      kor: {
        navAbout: "소개",
        navContent: "프로젝트 상세",
        navContact: "연락하기",
        darkToggle: "🌙",
        langToggle: "ENG"
      },
      eng: {
        navAbout: "About",
        navContent: "Project Details",
        navContact: "Contact",
        darkToggle: "☀️",
        langToggle: "KOR"
      }
    };

    function setLanguage(lang) {
      const t = translations[lang];
      document.getElementById('navAbout').textContent = t.navAbout;
      document.getElementById('navContent').textContent = t.navContent;
      document.getElementById('navContact').textContent = t.navContact;
      document.getElementById('darkModeToggle').textContent = t.darkToggle;
      document.getElementById('langToggle').textContent = t.langToggle;
      localStorage.setItem('lang', lang);
    }

    const langToggleBtn = document.getElementById('langToggle');
    langToggleBtn.addEventListener('click', () => {
      const currentLang = localStorage.getItem('lang') || 'kor';
      setLanguage(currentLang === 'kor' ? 'eng' : 'kor');
    });
    const savedLang = localStorage.getItem('lang') || 'kor';
    setLanguage(savedLang);
  </script>
</body>
</html>
